{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f399c87",
   "metadata": {},
   "source": [
    "## Summerizer input jpg --> .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1836c268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: DEV-2022-038.jpg | read 0.22s, prep 0.01s, ocr 2.42s\n",
      "Extracted: DEV-2022-039.jpg | read 0.14s, prep 0.01s, ocr 2.63s\n",
      "Extracted: DEV-2022-069.jpg | read 0.14s, prep 0.01s, ocr 2.09s\n",
      "Extracted: DEV-2022-070.jpg | read 0.13s, prep 0.01s, ocr 3.23s\n",
      "Extracted: DEV-2022-071.jpg | read 0.14s, prep 0.01s, ocr 3.16s\n",
      "Extracted: DEV-2022-072.jpg | read 0.14s, prep 0.01s, ocr 1.91s\n",
      "Extracted: DEV-2022-074.jpg | read 0.14s, prep 0.01s, ocr 3.27s\n",
      "Extracted: DEV-2022-084.jpg | read 0.14s, prep 0.01s, ocr 3.93s\n",
      "Extracted: DEV-2022-085.jpg | read 0.14s, prep 0.01s, ocr 3.40s\n",
      "Extracted: DEV-2022-108.jpg | read 0.13s, prep 0.01s, ocr 3.27s\n",
      "Extracted: DEV-2022-122.jpg | read 0.21s, prep 0.01s, ocr 2.80s\n",
      "\n",
      "✅ Done. Output: /Users/moon/Documents/dev-summarizer/dev-raw-ex/extracted_texts.txt\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "input_folder = \"/Users/moon/Documents/dev-summarizer/dev-raw-ex\"\n",
    "output_file  = \"/Users/moon/Documents/dev-summarizer/dev-raw-ex/extracted_texts.txt\"\n",
    "\n",
    "# Tesseract config: LSTM engine, assume block of text (psm 6). Try psm 4/6/7 depending on layout.\n",
    "TESS_CONFIG = r\"--oem 1 --psm 6\"\n",
    "\n",
    "def preprocess_fast(img_gray, max_w=1800):\n",
    "    # optional downscale for speed\n",
    "    h, w = img_gray.shape\n",
    "    if w > max_w:\n",
    "        scale = max_w / float(w)\n",
    "        img_gray = cv2.resize(img_gray, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "    # quick denoise + binarize\n",
    "    img_gray = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    "    _, thresh = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return thresh\n",
    "\n",
    "def extract_text(image_path):\n",
    "    t0 = time.perf_counter()\n",
    "    # read directly as grayscale (saves a conversion)\n",
    "    gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    t_read = time.perf_counter()\n",
    "\n",
    "    proc = preprocess_fast(gray)\n",
    "    t_prep = time.perf_counter()\n",
    "\n",
    "    text = pytesseract.image_to_string(proc, lang=\"eng\", config=TESS_CONFIG)\n",
    "    t_ocr = time.perf_counter()\n",
    "\n",
    "    return text.strip(), (t_read - t0, t_prep - t_read, t_ocr - t_prep)\n",
    "\n",
    "# --- only first 5 jpgs ---\n",
    "jpg_files = sorted([f for f in os.listdir(input_folder) if f.lower().endswith(\".jpg\")])#[:5]\n",
    "\n",
    "results = []\n",
    "for file in jpg_files:\n",
    "    path = os.path.join(input_folder, file)\n",
    "    text, (t_read, t_prep, t_ocr) = extract_text(path)\n",
    "    results.append(f\"\\n\\n===== {file} =====\\n{text}\")\n",
    "    print(f\"Extracted: {file} | read {t_read:.2f}s, prep {t_prep:.2f}s, ocr {t_ocr:.2f}s\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(results))\n",
    "\n",
    "print(f\"\\n✅ Done. Output: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245e2ca",
   "metadata": {},
   "source": [
    "## Making DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ab74f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        # --- date fallback: attempt to discover a date anywhere in the document if missing ---\\n        if required_labels:\\n            # canonical name used in your notebook: \"Date of Occurrence\"\\n            target = \"Date of Occurrence\"\\n            if target in required_labels and not record.get(target):\\n                # search header first, then body\\n                search_text = (head + \"\\n\" + body) if head else body\\n                # remove some noisy OCR chars that break matches\\n                search_text = re.sub(r\\'[^\\x00-\\x7f]+\\', \\' \\', search_text)\\n                m = DATE_RE.search(search_text)\\n                if m:\\n                    record[target] = normalize_date_str(m.group(1))\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ...existing code...\n",
    "import datetime, itertools, typing, re\n",
    "\n",
    "# tolerant date finder — picks first plausible date-like token and normalizes to YYYY-MM-DD if possible\n",
    "DATE_RE = re.compile(r'\\b(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}|[A-Za-z]{3,9}\\s+\\d{1,2},\\s*\\d{4})\\b')\n",
    "\n",
    "def normalize_date_str(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    for fmt in (\"%m/%d/%Y\",\"%m/%d/%y\",\"%d/%m/%Y\",\"%d/%m/%y\",\"%Y-%m-%d\",\"%b %d, %Y\",\"%B %d, %Y\"):\n",
    "        try:\n",
    "            dt = datetime.datetime.strptime(s, fmt)\n",
    "            return dt.date().isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # best effort: try splitting numeric parts\n",
    "    m = re.match(r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})', s)\n",
    "    if m:\n",
    "        mm, dd, yy = m.group(1), m.group(2), m.group(3)\n",
    "        if len(yy) == 2: yy = '20' + yy if int(yy) <= 50 else '19' + yy\n",
    "        try:\n",
    "            return datetime.date(int(yy), int(mm), int(dd)).isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return s  # fallback: return raw\n",
    "\n",
    "# Inject this small helper into extract_records post-loop: if a required date label is empty,\n",
    "# search the entire doc body for a date and assign the normalized value.\n",
    "# Add after building `record` (before extracted.append(...)):\n",
    "\n",
    "# Example patch to insert inside extract_records just before `extracted.append(dict(record))`:\n",
    "\"\"\"\n",
    "        # --- date fallback: attempt to discover a date anywhere in the document if missing ---\n",
    "        if required_labels:\n",
    "            # canonical name used in your notebook: \"Date of Occurrence\"\n",
    "            target = \"Date of Occurrence\"\n",
    "            if target in required_labels and not record.get(target):\n",
    "                # search header first, then body\n",
    "                search_text = (head + \"\\n\" + body) if head else body\n",
    "                # remove some noisy OCR chars that break matches\n",
    "                search_text = re.sub(r'[^\\x00-\\x7F]+', ' ', search_text)\n",
    "                m = DATE_RE.search(search_text)\n",
    "                if m:\n",
    "                    record[target] = normalize_date_str(m.group(1))\n",
    "\"\"\"\n",
    "\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bc6e7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re, unicodedata, difflib\n",
    "\n",
    "# --- helpers you already have (shown here for completeness) ---\n",
    "def normalize_text(s:str)->str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"：\", \":\").replace(\"|\", \":\").replace(\"—\", \"-\").replace(\"–\",\"-\")\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\r\\n?\", \"\\n\", s)\n",
    "    return s\n",
    "\n",
    "def is_labelish(line:str)->bool:\n",
    "    if len(line) < 2 or len(line) > 80: return False\n",
    "    if re.search(r\"\\s*[:\\-]\\s*$\", line): return True\n",
    "    if re.match(r\"^\\s*[\\w /()%#&.,]+?\\s*[:\\-]\\s+\\S\", line): return True\n",
    "    if re.match(r\"^[A-Z][A-Za-z0-9 /()%#&.,]{2,}$\", line) and len(line.split())<=6:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def norm_key(s:str)->str:\n",
    "    return re.sub(r\"[^a-z0-9]+\",\"\", s.lower())\n",
    "\n",
    "def best_canonical(label, canon_list, synonyms=None, thresh=0.82):\n",
    "    raw = label.strip()\n",
    "    if synonyms and raw in synonyms:\n",
    "        return synonyms[raw]\n",
    "    nk = norm_key(raw)\n",
    "    if synonyms:\n",
    "        for k,v in synonyms.items():\n",
    "            if norm_key(k)==nk: return v\n",
    "    if canon_list:\n",
    "        m = difflib.get_close_matches(raw, canon_list, n=1, cutoff=thresh)\n",
    "        if m: return m[0]\n",
    "        norm_map = {c:norm_key(c) for c in canon_list}\n",
    "        candidates = [c for c in canon_list if difflib.SequenceMatcher(None, nk, norm_map[c]).ratio()>=thresh]\n",
    "        if candidates: return candidates[0]\n",
    "    return raw\n",
    "\n",
    "# -------- core extractor (with required labels) --------\n",
    "def extract_records(\n",
    "    raw_text: str,\n",
    "    file_split_pattern: str = r\"^=+ .+?\\.jpg =+$\",\n",
    "    synonyms: dict | None = None,\n",
    "    required_labels: list[str] | None = None,\n",
    "):\n",
    "    text = normalize_text(raw_text)\n",
    "\n",
    "    # split into docs if you have separators; else one doc\n",
    "    parts = re.split(file_split_pattern, text, flags=re.MULTILINE)\n",
    "    headers = re.findall(file_split_pattern, text, flags=re.MULTILINE)\n",
    "    docs = []\n",
    "    for i,chunk in enumerate(parts):\n",
    "        if not chunk.strip(): \n",
    "            continue\n",
    "        header = headers[i-1] if i>0 and i-1 < len(headers) else \"\"\n",
    "        docs.append((header.strip(\"= \").strip(), chunk.strip()))\n",
    "    if not docs:\n",
    "        docs = [(\"\", text)]\n",
    "\n",
    "    # seed canonical labels with required ones so fuzzy matching prefers them\n",
    "    canonical_labels = list(dict.fromkeys(required_labels or []))\n",
    "\n",
    "    extracted = []\n",
    "    line_label_value = re.compile(r\"^\\s*([\\w /()%#&.,]+?)\\s*[:\\-]\\s*(.*)$\")\n",
    "\n",
    "    for head, body in docs:\n",
    "        record = defaultdict(str)\n",
    "        if head:\n",
    "            record[\"source_header\"] = head\n",
    "\n",
    "        lines = body.split(\"\\n\")\n",
    "        i = 0\n",
    "\n",
    "        while i < len(lines):\n",
    "            line = lines[i].rstrip()\n",
    "\n",
    "            # 1) \"Label: value\" on one line\n",
    "            m = line_label_value.match(line)\n",
    "            if m and is_labelish(m.group(1)):\n",
    "                raw_label = m.group(1).strip()\n",
    "                value = m.group(2).strip()\n",
    "                canon = best_canonical(raw_label, canonical_labels, synonyms)\n",
    "                if canon not in canonical_labels:\n",
    "                    canonical_labels.append(canon)\n",
    "                # collect continuation until next label\n",
    "                j = i+1\n",
    "                cont = []\n",
    "                while j < len(lines):\n",
    "                    nxt = lines[j].rstrip()\n",
    "                    if line_label_value.match(nxt) and is_labelish(line_label_value.match(nxt).group(1)):\n",
    "                        break\n",
    "                    if is_labelish(nxt) and nxt.strip().endswith(\":\"):\n",
    "                        break\n",
    "                    cont.append(nxt)\n",
    "                    j += 1\n",
    "                block = \"\\n\".join([value] + cont).strip()\n",
    "                record[canon] = (record[canon] + \"\\n\" + block).strip() if record[canon] else block\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "            # 2) \"Label:\" on its own line; value starts next line(s)\n",
    "            if is_labelish(line) and line.strip().endswith(\":\"):\n",
    "                raw_label = line.strip()[:-1].strip()\n",
    "                canon = best_canonical(raw_label, canonical_labels, synonyms)\n",
    "                if canon not in canonical_labels:\n",
    "                    canonical_labels.append(canon)\n",
    "                j = i+1\n",
    "                cont = []\n",
    "                while j < len(lines):\n",
    "                    nxt = lines[j].rstrip()\n",
    "                    if line_label_value.match(nxt) and is_labelish(line_label_value.match(nxt).group(1)):\n",
    "                        break\n",
    "                    if is_labelish(nxt) and nxt.strip().endswith(\":\"):\n",
    "                        break\n",
    "                    cont.append(nxt)\n",
    "                    j += 1\n",
    "                block = \"\\n\".join(cont).strip()\n",
    "                if block:\n",
    "                    record[canon] = (record[canon] + \"\\n\" + block).strip() if record[canon] else block\n",
    "                i = j\n",
    "                continue\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        # ensure required labels exist in every record\n",
    "        if required_labels:\n",
    "            for rl in required_labels:\n",
    "                record.setdefault(rl, \"\")\n",
    "\n",
    "        extracted.append(dict(record))\n",
    "\n",
    "    # also return canonical_labels with required ones at the front (unique order)\n",
    "    canonical_labels = list(dict.fromkeys((required_labels or []) + canonical_labels))\n",
    "    return extracted, canonical_labels\n",
    "\n",
    "\n",
    "# --------- learning the schema (optional) ----------\n",
    "def learn_schema(records, top_k=5):\n",
    "    freq = Counter()\n",
    "    for r in records:\n",
    "        for k in r.keys():\n",
    "            if k != \"source_header\":\n",
    "                freq[k]+=1\n",
    "    # descending by frequency then name\n",
    "    return [k for k,_ in freq.most_common(top_k)]\n",
    "\n",
    "\n",
    "def normalize_date_str(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    for fmt in (\"%m/%d/%Y\",\"%m/%d/%y\",\"%d/%m/%Y\",\"%d/%m/%y\",\"%Y-%m-%d\",\"%b %d, %Y\",\"%B %d, %Y\"):\n",
    "        try:\n",
    "            dt = datetime.datetime.strptime(s, fmt)\n",
    "            return dt.date().isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    # best effort: try splitting numeric parts\n",
    "    m = re.match(r'(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})', s)\n",
    "    if m:\n",
    "        mm, dd, yy = m.group(1), m.group(2), m.group(3)\n",
    "        if len(yy) == 2: yy = '20' + yy if int(yy) <= 50 else '19' + yy\n",
    "        try:\n",
    "            return datetime.date(int(yy), int(mm), int(dd)).isoformat()\n",
    "        except Exception:\n",
    "            pass\n",
    "    return s  # fallback: return raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = open(\"/Users/moon/Documents/dev-summarizer/dev-raw-ex/extracted_texts.txt\", \"r\", encoding=\"utf-8\").read()\n",
    "\n",
    "# Seed a few synonyms so minor wording changes still map together:\n",
    "synonyms = {\n",
    "    \"Deviation Number\": \"Deviation Number\",\n",
    "    \"De viation Number\": \"Deviation Number\",\n",
    "    \"Date of Occurrence\": \"Date of Occurrence\",\n",
    "    \"Document / Record Number\": \"Document / Record Number\",\n",
    "    \"Equipment Number\": \"Equipment Number\",\n",
    "    \"Project Number\": \"Project Number\",\n",
    "    \"Product Stock Number\": \"Product Stock Number\",\n",
    "    \"Lot Number\": \"Lot Number\",\n",
    "    \"Description of Deviation\": \"Description of Deviation\",\n",
    "    \"Description of Deviation or Non-conformance\": \"Description of Deviation or Non-conformance\",\n",
    "    \"Immediate Correction Employed\": \"Immediate Correction Employed\",\n",
    "    \"Immediate Correction\": \"Immediate Correction Employed\",\n",
    "    \"Corrective Action Required\": \"Corrective Action Required\",\n",
    "    \"Corrective Action\": \"Corrective Action\",\n",
    "    \"Corresponding CAPA Number\": \"CAPA Number\",\n",
    "    \"CAPA Number\": \"CAPA Number\",\n",
    "    \"No Impact Justification/Rationale\": \"No Impact Justification/Rationale\",\n",
    "    \"Client\": \"Client\",\n",
    "    \"Notification Required?\": \"Notification Required?\",\n",
    "    \"Deviation Type (Check All that Apply)\": \"Deviation Type\"\n",
    "}\n",
    "\n",
    "\n",
    "required = [\n",
    "    \"Deviation Number\",\n",
    "    \"Date of Occurrence\",\n",
    "    \"Document / Record Number\",\n",
    "    \"Equipment Number\",\n",
    "    \"Project Number\",\n",
    "    \"Product Stock Number\",\n",
    "    \"Lot Number\",\n",
    "    \"Description of Deviation or Non-conformance\",\n",
    "    \"Immediate Correction Employed\",\n",
    "    \"Corrective Action Required\",\n",
    "    \"Corrective Action\",\n",
    "    \"CAPA Number\",\n",
    "    \"No Impact Justification/Rationale\",\n",
    "    \"Client\",\n",
    "    \"Notification Required?\",\n",
    "]\n",
    "\n",
    "records, canon_labels = extract_records(raw, synonyms=synonyms, required_labels=required)\n",
    "\n",
    "# If you still want to learn extra labels (beyond required) for the CSV order:\n",
    "schema_learned = learn_schema(records, top_k=25)\n",
    "schema = list(dict.fromkeys(required + schema_learned))  # required first\n",
    "\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for rec in records:\n",
    "    row = {k: rec.get(k,\"\") for k in schema}\n",
    "    row[\"source_header\"] = rec.get(\"source_header\",\"\")\n",
    "    rows.append(row)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"deviation_db.csv\", index=False, encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6b35e8",
   "metadata": {},
   "source": [
    "## DB to summerizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7bbe305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: deviation_db_with_summary.csv\n",
      "Rows summarized: 11\n",
      "Columns used: ['Description of Deviation or Non', 'Immediate Correction Employed', 'Corrective Action Required', 'Corrective Action', 'Deviation Number', 'Document / Record Number', 'CAPA Number', 'Equipment Number', 'Deviation Type', 'No Impact Justification/Rationale', 'Processing', 'SOP Number 24', 'Written By', 'Deviation Type (Check Ail that Apply)', 'Protocol', 'source_header']\n"
     ]
    }
   ],
   "source": [
    "# make_summary_column.py\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "INPUT_CSV  = \"deviation_db.csv\"              # your existing DB\n",
    "OUTPUT_CSV = \"deviation_db_with_summary.csv\" # output path\n",
    "SUMMARY_COL = \"summary\"\n",
    "\n",
    "# ---- 1) Choose which columns to include ----\n",
    "# By default: include *all* object/text columns (auto-detected).\n",
    "# If you want to pin specific columns, set INCLUDE_COLS = [\"description\", \"immediate_correction\", ...]\n",
    "INCLUDE_COLS: List[str] = []   # leave empty to auto-detect\n",
    "\n",
    "# ---- 2) Summarizer wiring ----\n",
    "# We try to import summarize() from summarizer.py. If that fails, we fall back to a CLI call.\n",
    "SUMMARIZER_PATH = Path(\"summarizer.py\")\n",
    "\n",
    "def try_import_summarizer():\n",
    "    try:\n",
    "        import importlib.util\n",
    "        spec = importlib.util.spec_from_file_location(\"summarizer\", SUMMARIZER_PATH)\n",
    "        mod = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(mod)  # type: ignore\n",
    "        if hasattr(mod, \"summarize\") and callable(mod.summarize):\n",
    "            return mod.summarize\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def summarize_via_cli(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Fallback: run `python summarizer.py` and pass text on stdin.\n",
    "    Modify this if your CLI expects flags, e.g. ['python','summarizer.py','--mode','short'].\n",
    "    \"\"\"\n",
    "    try:\n",
    "        proc = subprocess.run(\n",
    "            [sys.executable, str(SUMMARIZER_PATH)],\n",
    "            input=text.encode(\"utf-8\"),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            check=False,\n",
    "        )\n",
    "        out = proc.stdout.decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "        if not out:\n",
    "            # If your CLI prints JSON, adjust parsing here.\n",
    "            out = \"(empty summary)\"\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        return f\"(summary error: {e})\"\n",
    "\n",
    "def build_row_text(row: pd.Series, cols: List[str]) -> str:\n",
    "    \"\"\"Concatenate selected columns -> a single text block.\n",
    "       Uses 'Label: value' lines, skips empties/NaN.\"\"\"\n",
    "    parts = []\n",
    "    for c in cols:\n",
    "        val = row.get(c, \"\")\n",
    "        if pd.isna(val) or str(val).strip() == \"\":\n",
    "            continue\n",
    "        parts.append(f\"{c}: {str(val).strip()}\")\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "\n",
    "# Load DB\n",
    "if not Path(INPUT_CSV).exists():\n",
    "    raise FileNotFoundError(f\"Could not find {INPUT_CSV}\")\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "# Auto-detect text columns if INCLUDE_COLS is empty\n",
    "if not INCLUDE_COLS:\n",
    "    text_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "else:\n",
    "    text_cols = [c for c in INCLUDE_COLS if c in df.columns]\n",
    "\n",
    "# Optional: put your important columns first so they appear earlier in the prompt\n",
    "priority = [\"deviation_number\",\"date_of_occurrence\",\"description\",\"immediate_correction\",\n",
    "            \"corrective_action_required\",\"corrective_action\",\"capa_number\",\"no_impact_rationale\"]\n",
    "ordered = [c for c in priority if c in text_cols] + [c for c in text_cols if c not in priority]\n",
    "text_cols = ordered\n",
    "\n",
    "# Prepare summarizer\n",
    "summarize_fn = try_import_summarizer()\n",
    "use_import = summarize_fn is not None\n",
    "if not use_import and not SUMMARIZER_PATH.exists():\n",
    "    raise FileNotFoundError(\"summarizer.py not found and import failed. Put it next to this script.\")\n",
    "\n",
    "# Create summary column\n",
    "summaries = []\n",
    "for idx, row in df.iterrows():\n",
    "    payload = build_row_text(row, text_cols)\n",
    "    if not payload:\n",
    "        summaries.append(\"\")\n",
    "        continue\n",
    "\n",
    "    if use_import:\n",
    "        try:\n",
    "            out = summarize_fn(payload)  # type: ignore\n",
    "        except Exception as e:\n",
    "            out = f\"(summary error: {e})\"\n",
    "    else:\n",
    "        out = summarize_via_cli(payload)\n",
    "\n",
    "    summaries.append(out)\n",
    "\n",
    "df[SUMMARY_COL] = summaries\n",
    "\n",
    "# Save output (keep a backup if overwriting)\n",
    "if Path(OUTPUT_CSV).resolve() == Path(INPUT_CSV).resolve():\n",
    "    Path(f\"{INPUT_CSV}.bak\").write_text(df.to_csv(index=False), encoding=\"utf-8\")\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved: {OUTPUT_CSV}\")\n",
    "print(f\"Rows summarized: {len(df)}\")\n",
    "print(f\"Columns used: {text_cols}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pharma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
